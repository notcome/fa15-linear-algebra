\documentclass{article}
\input{../general}
\usepackage{enumerate}

\MakeTitle{7}

\begin{document}
\maketitle

\begin{enumerate}[1.]
\item
First, we prove that $\vec{w}_i$ is orthogonal to $\vec{w}_1, \hdots, 
\vec{w}_{i-1}$ for $i \leq p$. Rewrite the definition of $w_i$ in terms of
the projection function:
\newcommand \ProjW[1]{\mathrm{proj}_{\vec{w}_{#1}}(\vec{v}_i)}
$$\vec{w}_i = \vec{v}_i - \sum_j\ProjW{j}$$
For some $j < i$:
$$
\begin{aligned}
(\vec{v}_i - \ProjW{j}) \cdot \vec{w}_j &= 0 \\
(\vec{w}_i + \sum_{k \neq j}\ProjW{k}) \cdot \vec{w}_j &= 0 \\
\vec{w}_i \cdot \vec{w}_j + \sum_{k \neq j} \ProjW{k} \cdot \vec{w}_j &= 0
\end{aligned}
$$
From the induction hypothesis we know that $\vec{w}_k \bot \vec{w}_j$ for
$k \neq j, k < i$, so $\ProjW{k} \cdot \vec{w}_j = 0$ for each such $k$.
Therefore, $\vec{w}_i \bot \vec{w}_j$.

Hence we have proven that $\vec{w}_i$ is orthogonal to $\vec{w}_1, \hdots,
\vec{w}_{i-1}$ for $i \leq p$. From this is trivial to see that $\vec{w}_1,
\hdots, \vec{w}_p$ is orthogonal to each other. In other words, they are
linearly independent\footnote{Textbook, \textbf{Proposition 2.4.17}}.

Since $\{\vec{v}_1, \hdots, \vec{v}_p\}$ is a basis of $E$, $E$'s maximal
linearly independent sets should have exactly $p$ elements. Therefore,
$\{\vec{w}_1, \hdots, \vec{w}_p\}$ forms a maximal linearly independent set of
$E$, and it is indeed an orthogonal basis of $E$. \rQED

\item
First, we prove the existence of such decomposition, which is equivalent to the statement that the union of one of  $E$'s basis and one of $E^{\bot}$'s basis is a basis for $R^n$. We pick an orthogonal basis for $E$ and another one for $E^{\bot}$. Assume that we still need some more vectors to form a basis for $R^n$. Apply the Gram--Schmidt process to these vectors, but ensure we first process vectors of the bases of $E$ and $E^{\top}$. In this way, since vectors of the two bases are orthogonal to each other, the process has no effect on these vectors---the subtracted vectors, or the projections, are always zero. After the process we get an orthogonal basis for $R^n$, with some of vectors forming a basis for $E$, some for $E^{\bot}$, and some other vectors. However, since those other vectors are orthogonal to vectors of a basis for $E$, they should be elements of $E^{\bot}$. Thus we get a contradiction, and we can safely say that an orthogonal decomposition always exist for vectors in $R^n$.

Next, we prove the uniqueness of such decomposition. Assume that we have $\vec{v} = \vec{x}_1 + \vec{y}_1 = \vec{x}_2 + \vec{y}_2$, where $\vec{x}_1, \vec{x}_2 \in E$, $\vec{y}_1, \vec{y}_2 \in E^{\bot}$, $\vec{x}_1 \neq \vec{x}_2$, and $\vec{y}_1 \neq \vec{y}_2$.
$$
\begin{aligned}
\vec{x}_1 + \vec{y}_1 &= \vec{x}_2 + \vec{y}_2 \\
\vec{x}_1 - \vec{x}_2 &= \vec{y}_2 - \vec{y}_1 \\
\end{aligned}
$$
The intersection of $E$ and $E^{\top}$ is $\{\vec{0}\}$, therefore both sides equal to the zero vector, or $\vec{x}_1 = \vec{x}_2$ and $\vec{y}_1 = \vec{y}_2$, hence another contradiction. As a result, orthogonal decomposition is unique. \rQED

\item
When $\vec{x}, \vec{y} \in \mathbb{R}^n$ is orthogonal to each other, we have:
\begin{align*}
\vec{x}^2 + \vec{y}^2 + 2\vec{x}\cdot\vec{y} &= (\vec{x} + \vec{y})^2\\
\vec{x}^2 + \vec{y}^2 + 2 \times 0 &= (\vec{x} + \vec{y})^2\\
\vec{x}^2 + \vec{y}^2 &= (\vec{x} + \vec{y})^2 \\
\vec{x}^2 &\leq (\vec{x} + \vec{y})^2 \\
|\vec{x}| &\leq |\vec{x} + \vec{y}|
\end{align*}

\newcommand \vecb[1]{\boldsymbol{\mathrm{#1}}}

Back to the question. $(\vecb{y} - \vecb{\hat{y}}) + (\vecb{\hat{y}} - \vecb{v}) = \vecb{y} - \vecb{v}$. Note that $\vecb{y} - \vecb{\hat{y}} \in E^{\bot}$, $\vecb{\hat{y}} - \vecb{v} \in E$, so these two vectors are orthogonal to each other other. Then, we can apply the above inequality and obtain $|\vecb{y} - \vecb{\hat{y}}| \leq \vecb{y} - \vecb{v}$. \rQED

\item
$E$ is the image of this matrix $A = \xmat{
-1& 3& 2\\
 3&-4& 1\\
 1&-1& 2\\
 1&-2&-1}$. $\widetilde{A} = \xmat{1&0&0\\0&1&0\\0&0&1\\0&0&0}$.
Since each column has a pivot, the three vectors form a basis. Name them 
$\vecb{v}_1, \vecb{v}_2, \vecb{v}_3$ and apply the Gram--Schmidt process:
$$\vecb{w}_1 = \vecb{v}_1 = \xmat{-1\\3\\1\\1} \quad
\vecb{w}_2 = \vecb{v}_2 - \frac{\vecb{v}_2 \cdot \vecb{w}_1}{\vecb{w}_1 \cdot \vecb{w}_1}\vecb{w}_1 = \xmat{5/3\\0\\1/3\\-2/3}$$$$
\vecb{w}_3 = \vecb{v}_3 - \frac{\vecb{v}_3 \cdot \vecb{w}_2}{\vecb{w}_2 \cdot \vecb{w}_2}\vecb{w}_2 - \frac{\vecb{v}_3 \cdot \vecb{w}_1}{\vecb{w}_1 \cdot \vecb{w}_1}\vecb{w}_1
% 1/6 w_1
% 7/5 w_2
= \xmat{-1/6\\1/2\\41/30\\-7/30}
$$

After normalization:
$$
\vecb{w}_1 = \xmat{-1\\3\\1\\1},\quad
\vecb{w}_2 = \xmat{5\\0\\1\\-2},\quad
\vecb{w}_3 = \xmat{-5\\15\\41\\-7}
$$

\item
\begin{align*}
\vecb{x}^{*}
&= (A^{\top}A)^{-1}A^{top}\vecb{b} \\
&= (A^{\top}A)^{-1} \xmat{20\\12\\12} \\
&= \xmat{4&8&10\\10&26&38\\8&20&26}^{-1} \xmat{20\\12\\12} \\
&= \frac{1}{16}
\xmat{21 & -11 & 2 \\
     -11 &  13 & -6\\
       2 &  -6 & 4}\xmat{20\\12\\12} \\
&= \xmat{10\\-6\\2}
\end{align*}

\item
Calculate the normal equation $A^{\top}A\vecb{x} = A^{\top}\vecb{b}$:
$$
\xmat{4&8&12\\8&20&32\\12&32&52} \vecb{x} = \xmat{12\\12\\12}
$$

Row reduce the augumented matrix and we can obtain $[\widetilde{A}|\widetilde{\vecb{b}}]$:
$$\xmat{1&0&-1&9\\0&1&2&-3\\0&0&0&0}$$

Let $\vecb{x} = \xmat{x\\y\\z}$, it should obey the following relations:
\begin{align*}
x-z&=9\\
y+2z&=-3
\end{align*}

The lack of a unique least-squares solution is because $A$'s columns are not linearly independent. Double the second column, add it with the first one, and we can obtain the third one.

\item
\begin{enumerate}
\item First we prove that $\ker A \subset \ker A^{\top}A$. For any $\vec{v} \in \ker A$, we have $(A^{\top}A)\vec{v} = A^{\top}(A\vec{v}) = A^{\top}\vec{0} = \vec{0}$, so $\vec{v} \in \ker A^{\top}A$.

Before moving on to prove that $\ker A^{\top}A \subset \ker A$, let's prove that the intersection between a matrix $M$'s row space $V$ and its kernel is $\{\vec{0}\}$. For a vector $\vec{v}$ being in the kernel of $M$, its inner dot products with every row of $M$ must be $\vec{0}$, namely $\vec{v}$ must be orthogonal to every row of $M$. Since $V$ is the span of all rows of $M$, $\vec{v}$ must be orthogonal to every vector in $V$. Therefore, we have proved that $M$'s kernel is a subset of $V^{\bot}$. Since $\vec{0}$ is an element of $M$'s kernel, $\ker M \cap V = \{\vec{0}\}$.

Let $B = A^{\top}$ and $B^{\top} = A$, we have $\ker A^{\top}A = \ker BB^{\top} = \{\vec{v} \in \mathbb{R}^n : B(B^{\top}\vec{v}) = \vec{0}\}$. For any $\vec{v} \in \ker A^{\top}A$, assume that $\vec{v} \notin \ker A$. Then, we have $B^{\top} \vec{v} \neq \vec{0}$ and $B(B^{\top}\vec{v}) = \vec{0}$, namely $B^{\top}\vec{v}$ is in both the row space and the kernel of $B$ and it is not $\vec{0}$, a contradiction to what we have proved. Therefore, $\ker A^{\top}A \subset \ker A$, and consequently, $\ker A = \ker A^{\top}A$. \rQED

\item
For any square matrix $M$, $\ker M = \{\vec{0}\}$ iff $M$ is invertible. If $\ker M = \{\vec{0}\}$, then each column of $M$ is linearly independent, and $M\vec{x} = \vec{y}$ has a unique solution. The reverse proof is similar.

If $\ker A = \{\vec{0}\}$, then $\ker A^{\top}A = \ker A = \{\vec{0}\}$, therefore $A^{\top}A$ is invertible. If $A^{\top}A$ is invertible, then $\ker A^{\top}A = \{\vec{0}\}$, and $\ker A = \ker A^{\top}A = \{\vec{0}\}$. \rQED
\end{enumerate}
\end{enumerate}
\end{document}
