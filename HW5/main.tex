\documentclass{article}
\input{../general}

\usepackage{amstext}
\usepackage{amssymb}
\usepackage{stmaryrd}
\DeclareFontFamily{OT1}{cmtex}{}
\DeclareFontShape{OT1}{cmtex}{m}{n}
  {<5><6><7><8>cmtex8
   <9>cmtex9
   <10><10.95><12><14.4><17.28><20.74><24.88>cmtex10}{}
\DeclareFontShape{OT1}{cmtex}{m}{it}
  {<-> ssub * cmtt/m/it}{}
\newcommand{\texfamily}{\fontfamily{cmtex}\selectfont}
\DeclareFontShape{OT1}{cmtt}{bx}{n}
  {<5><6><7><8>cmtt8
   <9>cmbtt9
   <10><10.95><12><14.4><17.28><20.74><24.88>cmbtt10}{}
\DeclareFontShape{OT1}{cmtex}{bx}{n}
  {<-> ssub * cmtt/bx/n}{}
\usepackage{polytable}

\MakeTitle{5}

\begin{document}
\maketitle

\gotosection{2}{4}
\subsection{Linear combinations, span, and linear independence}

\def \RRT{\quad\widetilde{}\quad}
\newcommand \spanft[2]{\mathrm{span}(#1,\hdots,#2)}

\begin{exercise}{2}
\begin{enumerate}
\item Since the matrix formed by these three vectors,  $\xmat{1&-2&-1\\2&1&1\\3&2&-1}$, can be row reduced to an identity matrix, they are linearly independent. As a result, they form a basis of $\mathbb{R}^3$. Because $\xmat{1\\2\\3\\} \cdot \xmat{-2\\1\\2} = 6 \neq 0$, these vector do not form an orthogonal set of vectors and therefore not a orthogonal basis. \rQED

\item Since
$$\xmat{4&3&2&4\\2&0&1&1\\1&4&4&2} \RRT \xmat{1&0&0&2/3\\0&1&0&2/3\\0&0&1&-1/3}$$
, the vector \emph{is} in the first span:
$$\xmat{4\\1\\2} = \frac{2}{3}\xmat{4\\2\\1} + \frac{2}{3}\xmat{3\\0\\4} - \frac{1}{3}\xmat{2\\1\\4}$$
But since
$$\xmat{4&3&5&4\\2&0&1&1\\1&4&4.5&2} \RRT \xmat{1&0&1/2&0\\0&1&1&0\\0&0&0&1}$$
, the vector \emph{is not} in the second span.
\end{enumerate}
\end{exercise}

\begin{exercise}{3}
Let $\Vect{v}_1 = \xmat{1\\1}, \Vect{v}_2 = \xmat{1,-1}$, then their corresponding normalized vectors are $\Vect{v}'_1 = \xmat{1/\sqrt{2}\\1/\sqrt{2}}, \Vect{v}'_2 = \xmat{1/\sqrt{2}\\-1/\sqrt{2}}$. $\Vect{v}'_1$ and $\Vect{v}'_2$ are linearly independent to each other: for any $\Vect{w} = a\Vect{v}_1 + b\Vect{v}_2 \in \mathbb{R}^2$, with $\Vect{v}'_1$ and $\Vect{v}'_2$ $\Vect{w}$ can and only can be written as $\sqrt{2}a\Vect{v}'_1 + \sqrt{2}b\Vect{v}'_2$. $\Vect{v}'_1 \cdot \Vect{v}'_2 = 0$, so they are orthogonal to each other. Hence $\Vect{v}'_1$ and $\Vect{v}'_2$ creates a orthonormal basis for $\mathbb{R}^2$.
\end{exercise}

\begin{exercise}{5}
\def \TheSpan{\spanft{\Vect{v}_1}{\Vect{v}_k}}

$\Vect{0} = 0\Vect{v}_1 + \hdots + 0\Vect{v}_k$, therefore, $\Vect{0} \in \TheSpan$. For any $\Vect{u}, \Vect{w} \in \TheSpan$, $\Vect{u} = \sum_{i=1}^ka_i\Vect{v}_i, \Vect{w} = \sum_{i=1}^kb_i\Vect{v}_i$, $\Vect{u} + \Vect{w} = \sum_{i=1}^k(a_i+b_i)\Vect{v}_i \in \TheSpan$. For any $\Vect{z} \in \TheSpan$, $\Vect{z} = \sum_{i=1}^kn_i\Vect{v}_i$, $c\Vect{z} = c\sum_{i=1}^kn_i\Vect{v}_i = \sum_{i=1}^kcn_i\Vect{v}_i \in \TheSpan$. Therefore, $\TheSpan$ is a subspace of $\mathbb{R}^n$. \rQED

Assuming there exists a subspace $V$ of $\mathbb{R}^n$ such that $V \subset \TheSpan$, $\Vect{v}_1, \hdots, \Vect{v}_k \in V$, and $V \neq \TheSpan$. For any $\Vect{u} \in \TheSpan$, since $\Vect{v}_1, \hdots, \Vect{v}_k \in V$ and $\Vect{u} = \sum_{i=1}^ka_i\Vect{v}_i$ for some $a_1, \hdots, a_k$, $\Vect{u} \in V$. Therefore, $V \subset \TheSpan$, $V = \TheSpan$, which contradicts to the assumption. \rQED
\end{exercise}

\begin{exercise}{7}
First, we prove that for any orthogonal matrix $A$, $A^{\top}A = I$.

Let $B = A^{\top}A$, from the formula of matrix multiplication we know that $B_{ij}$ is the dot product of $A^{\top}$'s $i$th row and $A$'s $j$th column. From the definition of transpose we also know that $A^{\top}$'s $i$th row is just $A$'s $i$th column. Let $a_i$ denote the vector represented by $A$'s $i$th column. Then, if $i = j$, $B_{ij} = B_{ii} = a_i \cdot a_i = 1$; if $i \neq j$, $B_{ij} = a_i \cdot a_j = 0$, since $a_i$ is orthogonal to $a_j$. Because all orthogonal entries of $B$ are one and all other entries are zero, $B$ is the identity matrix. Therefore, for any orthogonal matrix $A$, $A^{\top}A = I$.

Then, let's prove that for any square matrix $A$, if $A^{\top}A = I$, $A$ is orthogonal.

Again, let $a_i$ denote the vector represented by $A$'s $i$th column. We know that the dot product of $A^{\top}$'s $i$th row seen as a vector and $a_j$ is zero if $i \neq j$ and one if $i == j$. Since $A^{\top}$'s $i$th row is just $A$'s $i$th column, this means that $a_i \cdot a_i = 1$ and $a_i \cdot a_j = 0$ for all columns other than $a_i$. Therefore, each column of $A$ is normalized and is orthogonal to all other columns, Thus $A$ is orthogonal.

This completes the proof. \rQED
\end{exercise}

\begin{exercise}{10}
\begin{align*}
u\Vect{v}_1 + v\Vect{v}_2 &= x\Vect{e}_1 + y\Vect{e}_2 \\
u\Vect{v}_1 + v\Vect{v}_2 &= \xmat{x\\y} \\
\xmat{1&1\\1&3}\xmat{u\\v} &= \xmat{x\\y} \\
\xmat{u\\v} &= \xmat{1&1\\1&3}^{-1}\xmat{x\\y}
\end{align*}

Use the row reduction method to calculuate the matrix inverse:
$$\xmat{1&1&1&0\\1&3&0&1} \RRT \xmat{1&0&3/2&-1/2\\0&1&-1/2&1/2}$$

Therefore, $\xmat{u\\v} = \xmat{3/2&-1/2\\-1/2&1/2}\xmat{x\\y}$. For $\xmat{3\\-5}$, $u = 7, v = -4$:
$$\xmat{3\\-5} = 7\Vect{v}_1 - 4\Vect{v}_2 = 7\xmat{1\\1} - 4\xmat{1\\3}$$
\end{exercise}

\begin{exercise}{11}
\begin{enumerate}
\item
$n = 1$:
$$\left\{
\begin{aligned}
1 \cdot a_{0,1} + 1 \cdot a_{1,1} &= 1 \\
0 \cdot a_{0,1} + 1 \cdot a_{1,1} &= \frac{1}{2}
\end{aligned}
\right.$$

$n = 2$:
\newcommand \EqnII[4]{#1 \cdot a_{0,2} + #2a_{1,2} + #3 \cdot a_{2,2} &= #4}
$$\left\{
\begin{aligned}
\EqnII{1}{1\cdot}{1}{1} \\
\EqnII{0}{\frac{1}{2}}{1}{\frac{1}{2}} \\
\EqnII{0}{\frac{1}{4}}{1}{\frac{1}{3}}
\end{aligned}
\right.$$

$n = 3$:
\newcommand \EqnIII[5]{#1 \cdot a_{0,3} + #2a_{1,3} + #3a_{2,3} + #4 \cdot a_{3,3} &= #5}
$$\left\{
\begin{aligned}
\EqnIII{1}{1\cdot}{1\cdot}{1}{1} \\
\EqnIII{0}{\frac{1}{3}}{\frac{2}{3}}{1}{\frac{1}{2}} \\
\EqnIII{0}{\frac{1}{9}}{\frac{4}{9}}{1}{\frac{1}{3}} \\
\EqnIII{0}{\frac{1}{27}}{\frac{8}{27}}{1}{\frac{1}{4}}
\end{aligned}
\right.$$

\item \def \toApprox{\int_0^1\frac{dx}{x+1} \approx}
$n = 1$, $a_{0,1} = 1/2, a_{1,1} = 1/2$.
$$\toApprox \frac{1}{2} \times 1 + \frac{1}{2} \times \frac{1}{2} = \frac{3}{4} = 0.75$$

$n = 2$, $a_{0,2} = 1/6, a_{1,2} = 2/3, a_{2,2} = 1/6$.
$$\toApprox \frac{1}{6} \times 1 + \frac{2}{3} \times \frac{2}{3} + \frac{1}{6} \times \frac{1}{2} = \frac{25}{36} = 0.695$$

$n = 3$, $a_{0,3} = 1/8, a_{1,3} = 3/8, a_{0,3} = 3/8, a_{0,3} = 1/8$.
$$\toApprox \frac{1}{8} \times 1 + \frac{3}{8}\times\frac{3}{4} + \frac{3}{8} \times\frac{3}{5}+\frac{1}{8} \times \frac{1}{2} = 0.69375$$

\item $n = 8$, $[a_{0,8},\hdots,a_{8,8}] = $
$$
\xmat{\frac{989}{28350} & \frac{2944}{14175} & -\frac{464}{14175} & \frac{5248}{14175} & -\frac{454}{2835} & \frac{5248}{14175} & -\frac{464}{14175} & \frac{2944}{14175} & \frac{989}{28350}}
$$

One big difference is the existence of negative $a_{i,n}$.

The result is calculated by a Haskell program written by myself, using a simple matrix library lacking a row-reduction function. The code, including the row-reduction function, both of which are released to the public domain, is put at the last page of this homework assignment.
\end{enumerate}
\end{exercise}

\begin{exercise}{12}
\def \Vi{\Vect{v}_0}
\def \Va{\Vect{v}_1}
\def \Vb{\Vect{v}_2}
\def \Vc{\Vect{v}_3}

Let $\Vi,\Va,\Vb,\Vc \in \mathbb{R}^4$ denote the vectors associated with $I, A_t, A_t^2, A_t^3$ respectively. With matrix multiplication, we have:
$$\Vi = \cmat{1\\0\\0\\1} \quad \Va = \cmat{2\\t\\0\\2} \quad \Vb = \cmat{4\\4t\\0\\4} \quad \Vc = \cmat{8\\12t\\0\\8}$$

\begin{enumerate}
\item To figure out the dimension of the subspace $V_t$ spanned from $\Vi, \Va, \Vb, \Vc$, we can row reduce the matrix $\{\Vi, \Va, \Vb, \Vc\}$.
$$\cmat{1&2&4&8\\0&t&4t&12t\\0&0&0&0\\1&2&4&8} \RRT \cmat{1&2&4&8\\0&t&4t&12t\\0&0&0&0\\0&0&0&0}$$

The vector represented by a non-pivotal column can always be expanded to the linear combinations of vectors represented by pivotal columns preceding it. The rest vectors represented by pivotal columns in this matrix are linearly independent to each other. Since these ``pivotal vectors" can represent all other vectors in this matrix, the span of these vectors is the same as $V_t$. Therefore, the dimension of $V_t$ is just the number of pivotal columns of this row reduced matrix.

If $t = 0$, only the first column will have pivotal $1$, $\dim{V_t} = 1$. If $t \neq 0$, the second column can be further row reduce to a pivotal column but  the third and the fourth one cannot. Therefore, $\dim{V_t} = 2$. It's clear that in both scenarios these four vectors are not linearly independent.

\item For any matrix $B = \xmat{a&b\\c&d}$, $A_tB = \cmat{2a+ct&2b+dt\\2c&2d}$, $BA_t=\cmat{2a&at+2b\\2c&ct+2d}$. If $A_tB = BA_t$, we have:
$$\left\{\begin{aligned}
ct + 2a &= 2a \\
at + 2b &= 2b + dt \\
ct + 2d &= 2d
\end{aligned}\right.\quad \Longrightarrow \quad
\left\{\begin{aligned}
ct &= 0 \\
at &= dt
\end{aligned}\right.$$

If $t = 0$, $a, b, c, d$ can be any value, namely for any $\Vect{v} \in \mathbb{R}^4, \Vect{v} \in W_t$, or $W_t \supset \mathbb{R}^4$; by definition $W_t \subset \mathbb{R}^4$, therefore $W_t = \mathbb{R}^4$. As a consequence, $W_t$ is a trivial subspace and $\dim{W_t} = 4$.

If $t \neq 0$, $c = 0, a = d$,
$$B = \cmat{a\\b\\0\\a} = a\xmat{1\\0\\0\\1} + b\xmat{0\\1\\0\\0}$$

. Let $\Vect{u}_1 = \xmat{1\\0\\0\\1}, \Vect{u}_2 = \xmat{0\\1\\0\\0}$, $W_t = \{a\Vect{u}_1 + b\Vect{u}_2 : a, b \in \mathbb{R}\}$. Choose $a = b = 0$, $\Vect{0} \in W_t$. For any $a_1,b_1,a_2,b_2,c\in\mathbb{R}$, we have $a_1\Vect{u}_1 + b_1\Vect{u}_2 \in W_t$, $a_2\Vect{u}_1 + b_2\Vect{u}_2 \in W_t$, $a_1\Vect{u}_1 + b_1\Vect{u}_2 + a_2\Vect{u}_1 + b_2\Vect{u}_2  = (a_1+a_2)\Vect{u}_1 + (b_1+b_2)\Vect{u}_2 \in W_t$; $c(a_1\Vect{u}_1 + b_1\Vect{u}_2) = ca_1\Vect{u}_1 + cb_1\Vect{u}_2 \in W_t$. Therefore, $W_t$ is a subspace.

$\Vect{u}_1$ and $\Vect{u}_2$ are linearly independent. For any vector $\Vect{u} \in W_t$, $\Vect{u}$ can be written as a linear combination of $\Vect{u}_1$ and $\Vect{u}_2$; for any vector $\Vect{w}$ that can be written as a linear combination of $\Vect{u}_1$ and $\Vect{u}_2$, $\Vect{w} \in W_t$. Therefore, $\Vect{u}_1$ and $\Vect{u}_2$ form a basis of $W_t$, and $\dim{W_t} = 2$.

\item If $t=0$, $V_t = \{a\Vi:a\in\mathbb{R}\}$, $W_t = \mathbb{R}^4$. It's clear that $V_t \in W_t$. To prove that $V_t \neq W_t$, just pick a vector  $\Vect{u} \in \mathbb{R}^4$ whose second entry is nonzero.

If $t\neq 0$,  $V_t = \{a\Vi+b\Va:a,b\in\mathbb{R}\}$, $W_t = \{a\Vect{u}_1 + b\Vect{u}_2 : a, b \in \mathbb{R}\}$. To prove $V_t \subset W_t$, pick any $a,b \in \mathbb{R}$:
$$a\cmat{1\\0\\0\\1} + b\cmat{2\\t\\0\\2} = \cmat {a+2b\\bt\\0\\a+2b} = (a+2b)\Vect{u}_1 + b\Vect{u}_2 \in W_t$$

To figure out for what values of $t$, $V_t = W_t$, namely $V_t \supset W_t$, we can pick any $a, b=ct \in \mathbb{R}$:
$$a\cmat{1\\0\\0\\1} + b\cmat{0\\1\\0\\0} = \cmat{a\\ct\\0\\a} = (a-2c)\Vi + c\Va \in V_t$$

As long as $t \neq 0$, $V_t = W_t$.
\end{enumerate}
\end{exercise}

\newpage
\input{./haskell-src}
\end{document}
